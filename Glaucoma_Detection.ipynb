{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prisha90/Study-Oriented-Project/blob/main/Glaucoma_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYJFQHGDYL5q",
        "outputId": "503a54da-eccd-49e2-f83f-71708a85014c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from skimage import morphology\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTAW4G9oYM-x",
        "outputId": "6e3a0e1d-59ff-4bae-bd32-b83c11c56c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images in dataframe: 1020\n",
            "IMAGES_FOLDER found: /content/drive/MyDrive/SOP (Rajya Laksmi Ma'am)/Work Document/G1020/Images\n"
          ]
        }
      ],
      "source": [
        "IMAGES_FOLDER = \"/content/drive/MyDrive/SOP (Rajya Laksmi Ma'am)/Work Document/G1020/Images\"\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/SOP (Rajya Laksmi Ma'am)/Work Document/G1020/G1020.csv\")\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "image_size = (512, 512)\n",
        "\n",
        "print(f\"Total images in dataframe: {len(df)}\")\n",
        "\n",
        "if not os.path.exists(IMAGES_FOLDER):\n",
        "    print(f\"Error: IMAGES_FOLDER not found: {IMAGES_FOLDER}\")\n",
        "else:\n",
        "    print(f\"IMAGES_FOLDER found: {IMAGES_FOLDER}\")\n",
        "\n",
        "missing_images = [img for img in df['imageID'] if not os.path.exists(os.path.join(IMAGES_FOLDER, img))]\n",
        "if missing_images:\n",
        "    print(f\"Missing images: {len(missing_images)} images not found.\")\n",
        "\n",
        "def rotate_image(image, angle):\n",
        "    h, w = image.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    rot_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    rotated = cv2.warpAffine(image, rot_matrix, (w, h))\n",
        "    rotated = rotated.reshape(rotated.shape[0], rotated.shape[1], 1)\n",
        "    return rotated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdGppdDWYRRX"
      },
      "outputs": [],
      "source": [
        "for index, row in df.iterrows():\n",
        "    img_name = row['imageID']\n",
        "    label = row['binaryLabels']\n",
        "    img_path = os.path.join(IMAGES_FOLDER, img_name)\n",
        "\n",
        "    if os.path.exists(img_path):\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, image_size)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        img = cv2.GaussianBlur(img, (65, 65), 0)\n",
        "        img = img.reshape(image_size[0], image_size[1], 1)\n",
        "\n",
        "        images.append(img)\n",
        "        labels.append(label)\n",
        "\n",
        "        for angle in [45, 90, 170]:\n",
        "           rotated_img = rotate_image(img, angle)\n",
        "           images.append(rotated_img)\n",
        "           labels.append(label)\n",
        "    else:\n",
        "        print(f\"Image not found: {img_path}\")\n",
        "\n",
        "images = np.array(images) / 255.0\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9a-WSr9YbaG"
      },
      "outputs": [],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHwZ4Ci4YWPy"
      },
      "outputs": [],
      "source": [
        "def build_unet():\n",
        "    inputs = layers.Input((512, 512, 1))\n",
        "\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.BatchNormalization()(c1)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    c1 = layers.BatchNormalization()(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "    p1 = layers.Dropout(0.5)(p1)\n",
        "\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.BatchNormalization()(c2)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    c2 = layers.BatchNormalization()(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "    p2 = layers.Dropout(0.5)(p2)\n",
        "\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.BatchNormalization()(c3)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    c3 = layers.BatchNormalization()(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "    p3 = layers.Dropout(0.5)(p3)\n",
        "\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = layers.BatchNormalization()(c4)\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    c4 = layers.BatchNormalization()(c4)\n",
        "\n",
        "    u1 = layers.UpSampling2D((2, 2))(c4)\n",
        "    u1 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u1)\n",
        "    u1 = layers.BatchNormalization()(u1)\n",
        "    d1 = layers.Concatenate()([u1, c3])\n",
        "\n",
        "    u2 = layers.UpSampling2D((2, 2))(d1)\n",
        "    u2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u2)\n",
        "    u2 = layers.BatchNormalization()(u2)\n",
        "    d2 = layers.Concatenate()([u2, c2])\n",
        "\n",
        "    u3 = layers.UpSampling2D((2, 2))(d2)\n",
        "    u3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u3)\n",
        "    u3 = layers.BatchNormalization()(u3)\n",
        "    d3 = layers.Concatenate()([u3, c1])\n",
        "\n",
        "    od_output = layers.Conv2D(1, (1, 1), activation='sigmoid', name='optic_disc')(d3)\n",
        "    oc_output = layers.Conv2D(1, (1, 1), activation='sigmoid', name='optic_cup')(d3)\n",
        "\n",
        "    return models.Model(inputs, [od_output, oc_output])\n",
        "\n",
        "    unet = build_unet()\n",
        "\n",
        "    unet.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0004, momentum=0.95),\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1RnN3lgYha7"
      },
      "outputs": [],
      "source": [
        "od_train_dummy = np.zeros((len(X_train), 512, 512, 1))\n",
        "oc_train_dummy = np.zeros((len(X_train), 512, 512, 1))\n",
        "od_val_dummy = np.zeros((len(X_val), 512, 512, 1))\n",
        "oc_val_dummy = np.zeros((len(X_val), 512, 512, 1))\n",
        "\n",
        "unet.fit(X_train, [od_train_dummy, oc_train_dummy],\n",
        "         epochs=150, batch_size=2,\n",
        "         validation_data=(X_val, [od_val_dummy, oc_val_dummy]))\n",
        "\n",
        "od_pred, oc_pred = unet.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQOhIYs2Yiir"
      },
      "outputs": [],
      "source": [
        "def calculate_cdr(disc_mask, cup_mask):\n",
        "    disc_area = np.sum(disc_mask)\n",
        "    cup_area = np.sum(cup_mask)\n",
        "\n",
        "    return round((2 * cup_area) / disc_area, 4) if disc_area != 0 else 0\n",
        "\n",
        "def extract_isnt_quadrants(disc_mask, cup_mask):\n",
        "    disc_mask = disc_mask.squeeze()\n",
        "    cup_mask = cup_mask.squeeze()\n",
        "\n",
        "    disc_mask_rotated = np.rot90(disc_mask)\n",
        "    cup_mask_rotated = np.rot90(cup_mask)\n",
        "\n",
        "    nrr_mask = cv2.bitwise_xor(disc_mask_rotated, cup_mask_rotated)\n",
        "\n",
        "    height, width = disc_mask.shape\n",
        "    I = np.sum(nrr_mask[height//2:, :])\n",
        "    S = np.sum(nrr_mask[:height//2, :])\n",
        "    N = np.sum(nrr_mask[:, :width//2])\n",
        "    T = np.sum(nrr_mask[:, width//2:])\n",
        "\n",
        "    return round((1 + (I + S)) / (1 + (N + T)), 4) if (N + T) != 0 else 0\n",
        "\n",
        "def extract_blood_vessels(fundus_image):\n",
        "    if len(fundus_image.shape) == 2:\n",
        "        fundus_image = cv2.cvtColor(fundus_image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    gray_channel = fundus_image[:, :, 1].astype(np.uint8)\n",
        "    clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(6,6))\n",
        "    enhanced = clahe.apply(gray_channel)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    bottom_hat = cv2.morphologyEx(enhanced, cv2.MORPH_BLACKHAT, kernel)\n",
        "\n",
        "    threshold_value = 3.15 * np.std(bottom_hat)\n",
        "    _, vessel_mask = cv2.threshold(bottom_hat, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    height, width = vessel_mask.shape\n",
        "    I = np.sum(vessel_mask[height//2:, :])\n",
        "    S = np.sum(vessel_mask[:height//2, :])\n",
        "    N = np.sum(vessel_mask[:, :width//2])\n",
        "    T = np.sum(vessel_mask[:, width//2:])\n",
        "\n",
        "    return round((1 + (I + S)) / (1 + (N + T)), 4) if (N + T) != 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SSj2v2QYu8b"
      },
      "outputs": [],
      "source": [
        "cdr_test = np.array([calculate_cdr(od, oc) for od, oc in zip(od_pred, oc_pred)])\n",
        "isnt_test = np.array([extract_isnt_quadrants(od, oc) for od, oc in zip(od_pred, oc_pred)])\n",
        "bvr_test = np.array([extract_blood_vessels(img) for img in X_test])\n",
        "\n",
        "X_test_combined = np.hstack((cdr_test.reshape(-1,1), isnt_test.reshape(-1,1), bvr_test.reshape(-1,1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gUhsfifYzfw"
      },
      "outputs": [],
      "source": [
        "svm = SVC(kernel='rbf')\n",
        "svm.fit(X_train_combined, y_train)\n",
        "svm_predictions = svm.predict(X_test_combined)\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(3, 2), max_iter=120, activation='relu', solver='adam')\n",
        "mlp.fit(X_train_combined, y_train)\n",
        "mlp_predictions = mlp.predict(X_test_combined)\n",
        "\n",
        "adaboost = AdaBoostClassifier(n_estimators=50)\n",
        "adaboost.fit(X_train_combined, y_train)\n",
        "adaboost_predictions = adaboost.predict(X_test_combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07Oe5wyvY5ri"
      },
      "outputs": [],
      "source": [
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
        "adaboost_accuracy = accuracy_score(y_test, adaboost_predictions)\n",
        "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
        "print(f\"MLP Accuracy: {mlp_accuracy}\")\n",
        "print(f\"AdaBoost Accuracy: {adaboost_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}