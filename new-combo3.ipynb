{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5405aea",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-09T13:19:48.485067Z",
     "iopub.status.busy": "2025-04-09T13:19:48.484773Z",
     "iopub.status.idle": "2025-04-09T13:19:48.490097Z",
     "shell.execute_reply": "2025-04-09T13:19:48.489451Z"
    },
    "papermill": {
     "duration": 0.010268,
     "end_time": "2025-04-09T13:19:48.491421",
     "exception": false,
     "start_time": "2025-04-09T13:19:48.481153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import cv2\n",
    "# import os\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# from tensorflow.keras.applications import EfficientNetB0\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# IMG_SIZE = (224, 224)  \n",
    "# BATCH_SIZE = 32\n",
    "# RANDOM_STATE = 42\n",
    "\n",
    "# IMAGES_FOLDER = \"/kaggle/input/glaucoma-datasets/G1020/Images\"\n",
    "# df = pd.read_csv(\"/kaggle/input/glaucoma-datasets/G1020/G1020.csv\")\n",
    "\n",
    "# def load_data(df, IMG_SIZE):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "    \n",
    "#     for _, row in df.iterrows():\n",
    "#         img_name = row['imageID']\n",
    "#         label = row['binaryLabels']\n",
    "#         img_path = os.path.join(IMAGES_FOLDER, img_name)\n",
    "#         if os.path.exists(img_path):\n",
    "#             img = cv2.imread(img_path)\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#             img = cv2.resize(img, IMG_SIZE)\n",
    "#             images.append(img)\n",
    "#             labels.append(row['binaryLabels'])\n",
    "    \n",
    "#     return np.array(images), np.array(labels)\n",
    "\n",
    "# images, labels = load_data(df, IMG_SIZE)\n",
    "# images = images.astype('float32') / 255.0\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     images, labels, test_size=0.2, stratify=labels, random_state=RANDOM_STATE\n",
    "# )\n",
    "\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=30,\n",
    "#     brightness_range=[0.6, 1.4], \n",
    "#     zoom_range=0.3,\n",
    "#     horizontal_flip=True\n",
    "# )\n",
    "\n",
    "\n",
    "# augmented_images = []\n",
    "# augmented_labels = []\n",
    "# for x, y in zip(X_train, y_train):\n",
    "#     for _ in range(3):  \n",
    "#         augmented_images.append(datagen.random_transform(x))\n",
    "#         augmented_labels.append(y)\n",
    "        \n",
    "# X_train_aug = np.concatenate([X_train, np.array(augmented_images, dtype=np.float32)])\n",
    "# y_train_aug = np.concatenate([y_train, np.array(augmented_labels)])\n",
    "\n",
    "\n",
    "# # Feature extraction with ResNet50\n",
    "# def extract_features(images):\n",
    "#     # base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(*IMG_SIZE, 3))\n",
    "#     # x = GlobalAveragePooling2D()(base_model.output)\n",
    "#     # x = Dense(1024, activation='relu')(x)  \n",
    "#     # x = Dense(512, activation='relu')(x)  \n",
    "#     # x = Dropout(0.3)(x)\n",
    "#     # feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "#     base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(*IMG_SIZE, 3))\n",
    "#     x = GlobalAveragePooling2D()(base_model.output)\n",
    "#     x = Dense(1024, activation='relu')(x)\n",
    "#     x = Dropout(0.3)(x)\n",
    "#     feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "    \n",
    "#     features = feature_extractor.predict(images, batch_size=BATCH_SIZE, verbose=1)\n",
    "#     return features\n",
    "\n",
    "\n",
    "# print(\"Extracting training features...\")\n",
    "# train_features = extract_features(X_train)\n",
    "\n",
    "# print(\"Extracting test features...\")\n",
    "# test_features = extract_features(X_test)\n",
    "\n",
    "# # Normalize features\n",
    "# # scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "# train_features_aug = extract_features(X_train_aug)\n",
    "# X_train_features_aug = scaler.fit_transform(train_features_aug)\n",
    "# X_test_features = scaler.transform(test_features)\n",
    "\n",
    "# smote = SMOTE(random_state=RANDOM_STATE)\n",
    "# X_train_sm, y_train_sm = smote.fit_resample(X_train_features_aug, y_train_aug)\n",
    "# ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "# X_train_bal, y_train_bal = ros.fit_resample(X_train_features_aug, y_train_aug)\n",
    "\n",
    "# # svm_params = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1], 'kernel': ['rbf']}\n",
    "# # svm_grid = GridSearchCV(SVC(probability=True), svm_params, cv=5, n_jobs=-1, verbose=2)\n",
    "# # svm_grid.fit(X_train_bal, y_train_bal)\n",
    "# # print(f\"Best SVM parameters: {svm_grid.best_params_}\")\n",
    "# # y_pred = svm_grid.predict(X_test_features)\n",
    "# # y_prob = svm_grid.predict_proba(X_test_features)[:, 1]\n",
    "\n",
    "# svm = SVC(probability=True, class_weight={0: 1, 1: 3}, C=10, gamma=1, kernel='rbf')\n",
    "# svm.fit(X_train_bal, y_train_bal)\n",
    "# y_pred = svm.predict(X_test_features)\n",
    "# y_prob = svm.predict_proba(X_test_features)[:, 1]\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators=300, max_depth=20, class_weight={0: 1, 1: 3}, random_state=RANDOM_STATE)\n",
    "# rf.fit(X_train_bal, y_train_bal)\n",
    "# y_pred_rf = rf.predict(X_test_features)\n",
    "# y_prob_rf = rf.predict_proba(X_test_features)[:, 1]\n",
    "\n",
    "# cnn_model = Sequential([\n",
    "#     Conv2D(32, (3,3), activation='relu', input_shape=(*IMG_SIZE, 3)),\n",
    "#     MaxPooling2D(2,2),\n",
    "#     Conv2D(64, (3,3), activation='relu'),\n",
    "#     MaxPooling2D(2,2),\n",
    "#     Flatten(),\n",
    "#     Dense(512, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# cnn_model.compile(optimizer='adam', loss=focal_loss(), metrics=['accuracy'])\n",
    "# cnn_model.fit(X_train_bal, y_train_bal, epochs=10, batch_size=32)\n",
    "\n",
    "# y_pred_cnn = cnn.predict(X_test_features)\n",
    "# y_prob_cnn = cnn.predict_proba(X_test_features)[:, 1]\n",
    "\n",
    "# print(\"\\nClassification Report (SVM):\")\n",
    "# print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"Glaucoma\"]))\n",
    "# print(f\"\\nSVM Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "# print(f\"SVM AUC-ROC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "# logreg = LogisticRegression(max_iter=1000)\n",
    "# logreg.fit(X_train_features_aug, y_train_aug)\n",
    "# y_pred_logreg = logreg.predict(X_test_features)\n",
    "# print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_logreg):.4f}\")\n",
    "\n",
    "# print(\"\\nClassification Report (Random Forest):\")\n",
    "# print(classification_report(y_test, y_pred_rf, target_names=[\"Normal\", \"Glaucoma\"]))\n",
    "# print(f\"\\nRandom Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "# print(f\"Random Forest AUC-ROC: {roc_auc_score(y_test, y_prob_rf):.4f}\")\n",
    "\n",
    "# print(\"\\nClassification Report (CNN):\")\n",
    "# print(classification_report(y_test, y_pred_cnn, target_names=[\"Normal\", \"Glaucoma\"]))\n",
    "# print(f\"\\nCNN Accuracy: {accuracy_score(y_test, y_pred_cnn):.4f}\")\n",
    "# print(f\"CNN AUC-ROC: {roc_auc_score(y_test, y_prob_cnn):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae354a43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:19:48.496661Z",
     "iopub.status.busy": "2025-04-09T13:19:48.496391Z",
     "iopub.status.idle": "2025-04-09T13:28:37.399578Z",
     "shell.execute_reply": "2025-04-09T13:28:37.398523Z"
    },
    "papermill": {
     "duration": 528.907019,
     "end_time": "2025-04-09T13:28:37.400901",
     "exception": false,
     "start_time": "2025-04-09T13:19:48.493882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 240ms/step\n",
      "Extracting test features...\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 599ms/step - accuracy: 0.4805 - f1_metric: 0.3620 - loss: 1.8116 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 1.7369 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6250 - f1_metric: 0.4667 - loss: 1.3684 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 1.8114 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 286ms/step - accuracy: 0.4701 - f1_metric: 0.3959 - loss: 1.5950 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 4.1810 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4375 - f1_metric: 0.3817 - loss: 2.4756 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 4.2784 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 269ms/step - accuracy: 0.4590 - f1_metric: 0.3888 - loss: 1.6756 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 5.8344 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5000 - f1_metric: 0.5526 - loss: 1.4343 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 5.9107 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 283ms/step - accuracy: 0.4713 - f1_metric: 0.3717 - loss: 1.5069 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 8.2443 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4688 - f1_metric: 0.3261 - loss: 1.5301 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 8.3031 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 283ms/step - accuracy: 0.4906 - f1_metric: 0.3821 - loss: 1.4533 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 10.4229 - learning_rate: 2.0000e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4375 - f1_metric: 0.3099 - loss: 1.1492 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 10.5041 - learning_rate: 2.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 270ms/step - accuracy: 0.4553 - f1_metric: 0.3716 - loss: 1.5278 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 12.8165 - learning_rate: 2.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5000 - f1_metric: 0.3571 - loss: 1.1857 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 12.8938 - learning_rate: 2.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 284ms/step - accuracy: 0.5478 - f1_metric: 0.3448 - loss: 1.3366 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 14.6603 - learning_rate: 2.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3125 - f1_metric: 0.3281 - loss: 1.3518 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 14.7703 - learning_rate: 2.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 268ms/step - accuracy: 0.4738 - f1_metric: 0.3901 - loss: 1.4660 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 17.3199 - learning_rate: 2.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5625 - f1_metric: 0.2983 - loss: 1.1592 - val_accuracy: 0.2876 - val_f1_metric: 0.4413 - val_loss: 17.4466 - learning_rate: 4.0000e-06\n",
      "\n",
      "=== Enhanced Ensemble Model (Stacking) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.71      1.00      0.83       109\n",
      "    Glaucoma       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.71       153\n",
      "   macro avg       0.36      0.50      0.42       153\n",
      "weighted avg       0.51      0.71      0.59       153\n",
      "\n",
      "Accuracy: 0.7124\n",
      "AUC-ROC: 0.5206\n",
      "F1 Score: 0.5928\n",
      "Confusion Matrix:\n",
      "[[109   0]\n",
      " [ 44   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\n",
      "=== Enhanced CNN Model ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.00      0.00      0.00       109\n",
      "    Glaucoma       0.29      1.00      0.45        44\n",
      "\n",
      "    accuracy                           0.29       153\n",
      "   macro avg       0.14      0.50      0.22       153\n",
      "weighted avg       0.08      0.29      0.13       153\n",
      "\n",
      "Accuracy: 0.2876\n",
      "AUC-ROC: 0.5273\n",
      "F1 Score: 0.1285\n",
      "Confusion Matrix:\n",
      "[[  0 109]\n",
      " [  0  44]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout, Conv2D, MaxPooling2D, Flatten, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = (224, 224)  \n",
    "BATCH_SIZE = 32\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS = 50\n",
    "\n",
    "# Paths\n",
    "IMAGES_FOLDER = \"/kaggle/input/glaucoma-datasets/G1020/Images\"\n",
    "df = pd.read_csv(\"/kaggle/input/glaucoma-datasets/G1020/G1020.csv\")\n",
    "\n",
    "# Improved F1 metric\n",
    "def f1_metric(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(K.round(y_pred), 'float32')\n",
    "    \n",
    "    tp = K.sum(y_true * y_pred)\n",
    "    fp = K.sum((1 - y_true) * y_pred)\n",
    "    fn = K.sum(y_true * (1 - y_pred))\n",
    "\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n",
    "    return f1\n",
    "\n",
    "# Enhanced data loading with CLAHE and normalization\n",
    "def load_data(df, IMG_SIZE):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        img_name = row['imageID']\n",
    "        label = row['binaryLabels']\n",
    "        img_path = os.path.join(IMAGES_FOLDER, img_name)\n",
    "        if os.path.exists(img_path):\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, IMG_SIZE)\n",
    "            \n",
    "            # Enhanced preprocessing\n",
    "            lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "            cl = clahe.apply(l)\n",
    "            limg = cv2.merge((cl,a,b))\n",
    "            img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
    "            \n",
    "            # Normalize per image\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "            \n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load and preprocess data\n",
    "images, labels = load_data(df, IMG_SIZE)\n",
    "images = images.astype('float32')\n",
    "labels = labels.astype('float32')\n",
    "\n",
    "# Split data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images, labels, test_size=0.15, stratify=labels, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Enhanced augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='reflect'\n",
    ")\n",
    "\n",
    "# Feature extraction with EfficientNetB0\n",
    "def extract_features(images):\n",
    "    base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(*IMG_SIZE, 3))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "    \n",
    "    features = feature_extractor.predict(images, batch_size=BATCH_SIZE, verbose=1)\n",
    "    return features\n",
    "\n",
    "print(\"Extracting training features...\")\n",
    "train_features = extract_features(X_train)\n",
    "print(\"Extracting test features...\")\n",
    "test_features = extract_features(X_test)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_features = scaler.fit_transform(train_features)\n",
    "X_test_features = scaler.transform(test_features)\n",
    "\n",
    "# Improved class balancing\n",
    "sampler = make_pipeline(\n",
    "    SMOTE(sampling_strategy=0.8, random_state=RANDOM_STATE),\n",
    "    RandomUnderSampler(sampling_strategy=0.9, random_state=RANDOM_STATE)\n",
    ")\n",
    "X_train_bal, y_train_bal = sampler.fit_resample(X_train_features, y_train)\n",
    "\n",
    "# Enhanced ensemble model\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=500, max_depth=20, \n",
    "                                class_weight='balanced', random_state=RANDOM_STATE)),\n",
    "    ('svm', SVC(C=1, gamma='scale', kernel='rbf', \n",
    "               probability=True, class_weight='balanced', random_state=RANDOM_STATE)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=300, learning_rate=0.05,\n",
    "                                    max_depth=7, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(class_weight='balanced', C=0.1, penalty='l2', max_iter=1000),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack.fit(X_train_bal, y_train_bal)\n",
    "y_pred_stack = stack.predict(X_test_features)\n",
    "y_prob_stack = stack.predict_proba(X_test_features)[:, 1]\n",
    "\n",
    "# Enhanced CNN Model\n",
    "def create_cnn_model():\n",
    "    inputs = Input(shape=(*IMG_SIZE, 3))\n",
    "    \n",
    "    # Feature extraction\n",
    "    x = Conv2D(32, (3,3), activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(2,2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(2,2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(2,2)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Classification head\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', f1_metric]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Enhanced callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=15, restore_best_weights=True, monitor='val_f1_metric', mode='max'),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=7, monitor='val_f1_metric', mode='max', min_lr=1e-6),\n",
    "    ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_f1_metric', mode='max')\n",
    "]\n",
    "\n",
    "# Data generators for CNN\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cnn_model = create_cnn_model()\n",
    "history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    class_weight={0:1, 1:3}  # Higher weight for glaucoma cases\n",
    ")\n",
    "\n",
    "# Evaluate models\n",
    "print(\"\\n=== Enhanced Ensemble Model (Stacking) ===\")\n",
    "print(classification_report(y_test, y_pred_stack, target_names=[\"Normal\", \"Glaucoma\"]))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_prob_stack):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_stack, average='weighted'):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_stack))\n",
    "\n",
    "# CNN evaluation\n",
    "y_pred_cnn = (cnn_model.predict(X_test) > 0.5).astype(int)\n",
    "y_prob_cnn = cnn_model.predict(X_test)\n",
    "print(\"\\n=== Enhanced CNN Model ===\")\n",
    "print(classification_report(y_test, y_pred_cnn, target_names=[\"Normal\", \"Glaucoma\"]))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_cnn):.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_prob_cnn):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_cnn, average='weighted'):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d2c1db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:28:37.431206Z",
     "iopub.status.busy": "2025-04-09T13:28:37.430919Z",
     "iopub.status.idle": "2025-04-09T13:29:40.602341Z",
     "shell.execute_reply": "2025-04-09T13:29:40.600995Z"
    },
    "papermill": {
     "duration": 63.188766,
     "end_time": "2025-04-09T13:29:40.603817",
     "exception": false,
     "start_time": "2025-04-09T13:28:37.415051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class folders: ['/kaggle/input/drishtigs-retina-dataset-for-onh-segmentation/Training-20211018T055246Z-001/Training/Images/GLAUCOMA', '/kaggle/input/drishtigs-retina-dataset-for-onh-segmentation/Training-20211018T055246Z-001/Training/Images/NORMAL']\n",
      "Found class folders: ['/kaggle/input/drishtigs-retina-dataset-for-onh-segmentation/Test-20211018T060000Z-001/Test/Images/glaucoma', '/kaggle/input/drishtigs-retina-dataset-for-onh-segmentation/Test-20211018T060000Z-001/Test/Images/normal']\n",
      "Loaded 50 training images and 51 test images\n",
      "Training class distribution: (array([0, 1]), array([18, 32]))\n",
      "Test class distribution: (array([0, 1]), array([13, 38]))\n",
      "\n",
      "Extracting training features...\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 973ms/step\n",
      "Extracting test features...\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 835ms/step\n",
      "\n",
      "Training CNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.6318 - f1_metric: 0.6422 - loss: 6.1627 - val_accuracy: 0.2353 - val_f1_metric: 0.0451 - val_loss: 5.7159 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6250 - f1_metric: 0.5556 - loss: 6.5801 - val_accuracy: 0.2353 - val_f1_metric: 0.0451 - val_loss: 5.6787 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383ms/step - accuracy: 0.5029 - f1_metric: 0.5507 - loss: 7.0009 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 5.3749 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5000 - f1_metric: 0.4687 - loss: 5.8687 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 5.3715 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4693 - f1_metric: 0.5558 - loss: 7.0287 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 5.5531 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - f1_metric: 0.5000 - loss: 6.8389 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 5.6036 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5101 - f1_metric: 0.5238 - loss: 6.6016 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 5.8708 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - f1_metric: 0.5000 - loss: 7.5623 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 5.9161 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5216 - f1_metric: 0.5244 - loss: 6.6274 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 6.0980 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8750 - f1_metric: 0.5556 - loss: 6.0798 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 6.1185 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.6000 - f1_metric: 0.5465 - loss: 6.3939 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 6.3396 - learning_rate: 2.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6250 - f1_metric: 0.5556 - loss: 5.7797 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 6.3771 - learning_rate: 2.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.6508 - f1_metric: 0.5006 - loss: 5.9324 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 6.6031 - learning_rate: 2.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7500 - f1_metric: 0.7292 - loss: 6.3723 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 6.6246 - learning_rate: 2.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.6646 - f1_metric: 0.3695 - loss: 6.0273 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 6.8338 - learning_rate: 2.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3750 - f1_metric: 0.5556 - loss: 6.1646 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 6.8684 - learning_rate: 2.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6546 - f1_metric: 0.5614 - loss: 6.4089 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 7.0520 - learning_rate: 2.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7500 - f1_metric: 0.6250 - loss: 5.6566 - val_accuracy: 0.7451 - val_f1_metric: 0.7400 - val_loss: 7.0761 - learning_rate: 4.0000e-06\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\n",
      "=== CNN Model Results ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.00      0.00      0.00        13\n",
      "    Glaucoma       0.75      1.00      0.85        38\n",
      "\n",
      "    accuracy                           0.75        51\n",
      "   macro avg       0.37      0.50      0.43        51\n",
      "weighted avg       0.56      0.75      0.64        51\n",
      "\n",
      "Accuracy: 0.7451\n",
      "AUC: 0.5769\n",
      "F1 Score: 0.6363\n",
      "Confusion Matrix:\n",
      "[[ 0 13]\n",
      " [ 0 38]]\n",
      "\n",
      "Training traditional ML models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest Results ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.00      0.00      0.00        13\n",
      "    Glaucoma       0.75      1.00      0.85        38\n",
      "\n",
      "    accuracy                           0.75        51\n",
      "   macro avg       0.37      0.50      0.43        51\n",
      "weighted avg       0.56      0.75      0.64        51\n",
      "\n",
      "Accuracy: 0.7451\n",
      "AUC: 0.4909\n",
      "F1 Score: 0.6363\n",
      "Confusion Matrix:\n",
      "[[ 0 13]\n",
      " [ 0 38]]\n",
      "\n",
      "=== SVM Results ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.00      0.00      0.00        13\n",
      "    Glaucoma       0.75      1.00      0.85        38\n",
      "\n",
      "    accuracy                           0.75        51\n",
      "   macro avg       0.37      0.50      0.43        51\n",
      "weighted avg       0.56      0.75      0.64        51\n",
      "\n",
      "Accuracy: 0.7451\n",
      "AUC: 0.5000\n",
      "F1 Score: 0.6363\n",
      "Confusion Matrix:\n",
      "[[ 0 13]\n",
      " [ 0 38]]\n",
      "\n",
      "=== Logistic Regression Results ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.25      1.00      0.41        13\n",
      "    Glaucoma       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.25        51\n",
      "   macro avg       0.13      0.50      0.20        51\n",
      "weighted avg       0.06      0.25      0.10        51\n",
      "\n",
      "Accuracy: 0.2549\n",
      "AUC: 0.4008\n",
      "F1 Score: 0.1036\n",
      "Confusion Matrix:\n",
      "[[13  0]\n",
      " [38  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout, Conv2D, MaxPooling2D, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 8\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS = 50\n",
    "\n",
    "# Custom F1 metric\n",
    "def f1_metric(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(K.round(y_pred), 'float32')\n",
    "    tp = K.sum(y_true * y_pred)\n",
    "    fp = K.sum((1 - y_true) * y_pred)\n",
    "    fn = K.sum(y_true * (1 - y_pred))\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "    f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n",
    "    return f1\n",
    "\n",
    "def load_images_from_folders(base_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    class_mapping = {'normal': 0, 'glaucoma': 1}\n",
    "    found_folders = {}\n",
    "\n",
    "    # Walk recursively to find class folders\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for folder in dirs:\n",
    "            folder_lower = folder.lower()\n",
    "            if folder_lower in class_mapping:\n",
    "                full_path = os.path.join(root, folder)\n",
    "                found_folders[full_path] = class_mapping[folder_lower]\n",
    "\n",
    "    if not found_folders:\n",
    "        raise ValueError(f\"No valid class folders ('normal', 'glaucoma') found in {base_path}\")\n",
    "\n",
    "    print(f\"Found class folders: {list(found_folders.keys())}\")\n",
    "\n",
    "    for class_path, class_label in found_folders.items():\n",
    "        for fname in os.listdir(class_path):\n",
    "            if fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                path = os.path.join(class_path, fname)\n",
    "                try:\n",
    "                    img = cv2.imread(path)\n",
    "                    if img is None:\n",
    "                        print(f\"Warning: Could not read image {path}\")\n",
    "                        continue\n",
    "                        \n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, IMG_SIZE)\n",
    "\n",
    "                    # Enhanced preprocessing\n",
    "                    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "                    l, a, b = cv2.split(lab)\n",
    "                    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "                    cl = clahe.apply(l)\n",
    "                    limg = cv2.merge((cl, a, b))\n",
    "                    img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "                    # Normalize and enhance contrast\n",
    "                    img = img.astype('float32') / 255.0\n",
    "                    img = (img - img.mean()) / (img.std() + 1e-7)\n",
    "\n",
    "                    images.append(img)\n",
    "                    labels.append(class_label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        raise ValueError(f\"No valid images found in {base_path}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Define paths based on your structure\n",
    "train_path = \"/kaggle/input/drishtigs-retina-dataset-for-onh-segmentation/Training-20211018T055246Z-001/Training\"\n",
    "test_path = \"/kaggle/input/drishtigs-retina-dataset-for-onh-segmentation/Test-20211018T060000Z-001/Test\"\n",
    "\n",
    "# Load data with enhanced error handling\n",
    "try:\n",
    "    X_train, y_train = load_images_from_folders(train_path)\n",
    "    X_test, y_test = load_images_from_folders(test_path)\n",
    "    print(f\"Loaded {len(X_train)} training images and {len(X_test)} test images\")\n",
    "    print(\"Training class distribution:\", np.unique(y_train, return_counts=True))\n",
    "    print(\"Test class distribution:\", np.unique(y_test, return_counts=True))\n",
    "    \n",
    "    # Check if we have at least 2 classes\n",
    "    if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:\n",
    "        raise ValueError(\"Dataset must contain both normal and glaucoma cases for classification\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading data: {str(e)}\")\n",
    "    print(\"\\nDirectory structure:\")\n",
    "    for root, dirs, files in os.walk(\"/kaggle/input\"):\n",
    "        print(root)\n",
    "    raise\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='reflect'\n",
    ")\n",
    "\n",
    "# Feature extraction with EfficientNetB0\n",
    "def extract_features(images):\n",
    "    base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(*IMG_SIZE, 3))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    feature_model = Model(inputs=base_model.input, outputs=x)\n",
    "    return feature_model.predict(images, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "# Extract features\n",
    "print(\"\\nExtracting training features...\")\n",
    "train_features = extract_features(X_train)\n",
    "print(\"Extracting test features...\")\n",
    "test_features = extract_features(X_test)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_features)\n",
    "X_test_scaled = scaler.transform(test_features)\n",
    "\n",
    "# CNN Model\n",
    "def create_cnn_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(*IMG_SIZE, 3)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2,2),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2,2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2,2),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', f1_metric]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=15, restore_best_weights=True, monitor='val_f1_metric', mode='max'),\n",
    "    ReduceLROnPlateau(factor=0.2, patience=7, monitor='val_f1_metric', mode='max', min_lr=1e-6),\n",
    "    ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_f1_metric', mode='max')\n",
    "]\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\nTraining CNN model...\")\n",
    "cnn_model = create_cnn_model()\n",
    "history = cnn_model.fit(\n",
    "    train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    class_weight={0:1, 1:2}  # Adjust weights based on class imbalance\n",
    ")\n",
    "\n",
    "# Evaluate CNN\n",
    "y_pred_cnn = (cnn_model.predict(X_test) > 0.5).astype(int)\n",
    "y_prob_cnn = cnn_model.predict(X_test)\n",
    "print(\"\\n=== CNN Model Results ===\")\n",
    "print(classification_report(y_test, y_pred_cnn, target_names=[\"Normal\", \"Glaucoma\"]))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_cnn):.4f}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_prob_cnn):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_cnn, average='weighted'):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_cnn))\n",
    "\n",
    "# Traditional ML models\n",
    "print(\"\\nTraining traditional ML models...\")\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=RANDOM_STATE),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=RANDOM_STATE),\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    print(f\"\\n=== {name} Results ===\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"Normal\", \"Glaucoma\"]))\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    if y_prob is not None:\n",
    "        print(f\"AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1655371,
     "sourceId": 2716645,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2296461,
     "sourceId": 3863247,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 144899194,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 598.157048,
   "end_time": "2025-04-09T13:29:44.006780",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-09T13:19:45.849732",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
